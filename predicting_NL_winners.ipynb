{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "81ed0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c36871f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reading in team data\n",
    "data = pd.read_csv(\"team_data.csv\")\n",
    "\n",
    "#changing binary outcome to 0 and 1\n",
    "data.LgWin = data.LgWin.replace(['N', 'Y'], [0, 1])\n",
    "\n",
    "#data2 is all the data from 1995-2015 and NL only\n",
    "data2 = data[(data.yearID >= 1995) & (data.yearID <= 2015) & (data.lgID == 'NL')]\n",
    "\n",
    "#columns that I want to use to predict LgWin: I want to use fielding percentage, stolen bases, walks, hits \n",
    "cols = ['LgWin', 'FP', 'SB', 'BB', 'H', 'HR', 'ERA', 'R', 'X2B', 'X3B']\n",
    "\n",
    "#storing all relevant columns into df, then splitting into training and validation sets\n",
    "df = data2[cols]\n",
    "\n",
    "df_train = df.sample(frac=0.75, random_state=0)\n",
    "df_valid = df.drop(df_train.index)\n",
    "\n",
    "X_train = df_train.drop('LgWin', axis=1)\n",
    "X_valid = df_valid.drop('LgWin', axis=1)\n",
    "y_train = df_train['LgWin']\n",
    "y_valid = df_valid['LgWin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4da9ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8835 - binary_accuracy: 0.5020 - val_loss: 0.6552 - val_binary_accuracy: 0.6585\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8652 - binary_accuracy: 0.5347 - val_loss: 0.5477 - val_binary_accuracy: 0.8415\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8252 - binary_accuracy: 0.5388 - val_loss: 0.5204 - val_binary_accuracy: 0.9390\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7364 - binary_accuracy: 0.5551 - val_loss: 0.5022 - val_binary_accuracy: 0.9390\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7848 - binary_accuracy: 0.6245 - val_loss: 0.5132 - val_binary_accuracy: 0.9512\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7709 - binary_accuracy: 0.6571 - val_loss: 0.5124 - val_binary_accuracy: 0.9634\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6808 - binary_accuracy: 0.6408 - val_loss: 0.4786 - val_binary_accuracy: 0.9634\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6752 - binary_accuracy: 0.6531 - val_loss: 0.4214 - val_binary_accuracy: 0.9634\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6344 - binary_accuracy: 0.7020 - val_loss: 0.4175 - val_binary_accuracy: 0.9634\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6580 - binary_accuracy: 0.7592 - val_loss: 0.4607 - val_binary_accuracy: 0.9634\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5948 - binary_accuracy: 0.7796 - val_loss: 0.5000 - val_binary_accuracy: 0.9634\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5690 - binary_accuracy: 0.7837 - val_loss: 0.5397 - val_binary_accuracy: 0.9268\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5650 - binary_accuracy: 0.7878 - val_loss: 0.5511 - val_binary_accuracy: 0.9146\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5631 - binary_accuracy: 0.8163 - val_loss: 0.5411 - val_binary_accuracy: 0.9512\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5408 - binary_accuracy: 0.8122 - val_loss: 0.5223 - val_binary_accuracy: 0.9634\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5013 - binary_accuracy: 0.8449 - val_loss: 0.5022 - val_binary_accuracy: 0.9634\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5376 - binary_accuracy: 0.8408 - val_loss: 0.4871 - val_binary_accuracy: 0.9634\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5108 - binary_accuracy: 0.8653 - val_loss: 0.4648 - val_binary_accuracy: 0.9634\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4744 - binary_accuracy: 0.8816 - val_loss: 0.4367 - val_binary_accuracy: 0.9634\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4676 - binary_accuracy: 0.8898 - val_loss: 0.4133 - val_binary_accuracy: 0.9634\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4552 - binary_accuracy: 0.8980 - val_loss: 0.3892 - val_binary_accuracy: 0.9634\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4695 - binary_accuracy: 0.8857 - val_loss: 0.3683 - val_binary_accuracy: 0.9634\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4408 - binary_accuracy: 0.8816 - val_loss: 0.3539 - val_binary_accuracy: 0.9634\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4150 - binary_accuracy: 0.9061 - val_loss: 0.3378 - val_binary_accuracy: 0.9634\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3937 - binary_accuracy: 0.8898 - val_loss: 0.3244 - val_binary_accuracy: 0.9634\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4255 - binary_accuracy: 0.9143 - val_loss: 0.3107 - val_binary_accuracy: 0.9634\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3987 - binary_accuracy: 0.8980 - val_loss: 0.2972 - val_binary_accuracy: 0.9634\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4047 - binary_accuracy: 0.9061 - val_loss: 0.2836 - val_binary_accuracy: 0.9634\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3771 - binary_accuracy: 0.9143 - val_loss: 0.2721 - val_binary_accuracy: 0.9634\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3997 - binary_accuracy: 0.8980 - val_loss: 0.2607 - val_binary_accuracy: 0.9634\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3501 - binary_accuracy: 0.9102 - val_loss: 0.2479 - val_binary_accuracy: 0.9634\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3472 - binary_accuracy: 0.9143 - val_loss: 0.2359 - val_binary_accuracy: 0.9634\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3305 - binary_accuracy: 0.9224 - val_loss: 0.2251 - val_binary_accuracy: 0.9634\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3606 - binary_accuracy: 0.9102 - val_loss: 0.2164 - val_binary_accuracy: 0.9634\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3296 - binary_accuracy: 0.9143 - val_loss: 0.2084 - val_binary_accuracy: 0.9634\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3241 - binary_accuracy: 0.9224 - val_loss: 0.2013 - val_binary_accuracy: 0.9634\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3288 - binary_accuracy: 0.9102 - val_loss: 0.1953 - val_binary_accuracy: 0.9634\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3160 - binary_accuracy: 0.9265 - val_loss: 0.1911 - val_binary_accuracy: 0.9634\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3266 - binary_accuracy: 0.9102 - val_loss: 0.1881 - val_binary_accuracy: 0.9634\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3311 - binary_accuracy: 0.9143 - val_loss: 0.1866 - val_binary_accuracy: 0.9634\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3138 - binary_accuracy: 0.9184 - val_loss: 0.1848 - val_binary_accuracy: 0.9634\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3011 - binary_accuracy: 0.9102 - val_loss: 0.1832 - val_binary_accuracy: 0.9634\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3024 - binary_accuracy: 0.9224 - val_loss: 0.1820 - val_binary_accuracy: 0.9634\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2859 - binary_accuracy: 0.9224 - val_loss: 0.1801 - val_binary_accuracy: 0.9634\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2823 - binary_accuracy: 0.9184 - val_loss: 0.1790 - val_binary_accuracy: 0.9634\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2930 - binary_accuracy: 0.9143 - val_loss: 0.1775 - val_binary_accuracy: 0.9634\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2854 - binary_accuracy: 0.9184 - val_loss: 0.1762 - val_binary_accuracy: 0.9634\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3059 - binary_accuracy: 0.9184 - val_loss: 0.1750 - val_binary_accuracy: 0.9634\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2656 - binary_accuracy: 0.9265 - val_loss: 0.1740 - val_binary_accuracy: 0.9634\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2867 - binary_accuracy: 0.9224 - val_loss: 0.1730 - val_binary_accuracy: 0.9634\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2866 - binary_accuracy: 0.9061 - val_loss: 0.1718 - val_binary_accuracy: 0.9634\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2850 - binary_accuracy: 0.9224 - val_loss: 0.1704 - val_binary_accuracy: 0.9634\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2733 - binary_accuracy: 0.9143 - val_loss: 0.1689 - val_binary_accuracy: 0.9634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2758 - binary_accuracy: 0.9102 - val_loss: 0.1677 - val_binary_accuracy: 0.9634\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2692 - binary_accuracy: 0.9224 - val_loss: 0.1665 - val_binary_accuracy: 0.9634\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2885 - binary_accuracy: 0.9265 - val_loss: 0.1656 - val_binary_accuracy: 0.9634\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2686 - binary_accuracy: 0.9224 - val_loss: 0.1648 - val_binary_accuracy: 0.9634\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2850 - binary_accuracy: 0.9224 - val_loss: 0.1646 - val_binary_accuracy: 0.9634\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2840 - binary_accuracy: 0.9224 - val_loss: 0.1643 - val_binary_accuracy: 0.9634\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3044 - binary_accuracy: 0.9224 - val_loss: 0.1645 - val_binary_accuracy: 0.9634\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2644 - binary_accuracy: 0.9224 - val_loss: 0.1645 - val_binary_accuracy: 0.9634\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2733 - binary_accuracy: 0.9224 - val_loss: 0.1646 - val_binary_accuracy: 0.9634\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3026 - binary_accuracy: 0.9184 - val_loss: 0.1645 - val_binary_accuracy: 0.9634\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3036 - binary_accuracy: 0.9224 - val_loss: 0.1644 - val_binary_accuracy: 0.9634\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2551 - binary_accuracy: 0.9224 - val_loss: 0.1644 - val_binary_accuracy: 0.9634\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2922 - binary_accuracy: 0.9184 - val_loss: 0.1645 - val_binary_accuracy: 0.9634\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2524 - binary_accuracy: 0.9265 - val_loss: 0.1645 - val_binary_accuracy: 0.9634\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2598 - binary_accuracy: 0.9265 - val_loss: 0.1647 - val_binary_accuracy: 0.9634\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2613 - binary_accuracy: 0.9265 - val_loss: 0.1648 - val_binary_accuracy: 0.9634\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2754 - binary_accuracy: 0.9224 - val_loss: 0.1650 - val_binary_accuracy: 0.9634\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2645 - binary_accuracy: 0.9265 - val_loss: 0.1652 - val_binary_accuracy: 0.9634\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2783 - binary_accuracy: 0.9224 - val_loss: 0.1654 - val_binary_accuracy: 0.9634\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2665 - binary_accuracy: 0.9265 - val_loss: 0.1656 - val_binary_accuracy: 0.9634\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2683 - binary_accuracy: 0.9265 - val_loss: 0.1658 - val_binary_accuracy: 0.9634\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2662 - binary_accuracy: 0.9265 - val_loss: 0.1660 - val_binary_accuracy: 0.9634\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2508 - binary_accuracy: 0.9224 - val_loss: 0.1662 - val_binary_accuracy: 0.9634\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2817 - binary_accuracy: 0.9224 - val_loss: 0.1664 - val_binary_accuracy: 0.9634\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2812 - binary_accuracy: 0.9265 - val_loss: 0.1665 - val_binary_accuracy: 0.9634\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2658 - binary_accuracy: 0.9265 - val_loss: 0.1667 - val_binary_accuracy: 0.9634\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2630 - binary_accuracy: 0.9265 - val_loss: 0.1669 - val_binary_accuracy: 0.9634\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2872 - binary_accuracy: 0.9224 - val_loss: 0.1674 - val_binary_accuracy: 0.9634\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2623 - binary_accuracy: 0.9224 - val_loss: 0.1679 - val_binary_accuracy: 0.9634\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2623 - binary_accuracy: 0.9265 - val_loss: 0.1683 - val_binary_accuracy: 0.9634\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2641 - binary_accuracy: 0.9265 - val_loss: 0.1688 - val_binary_accuracy: 0.9634\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2576 - binary_accuracy: 0.9265 - val_loss: 0.1692 - val_binary_accuracy: 0.9634\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2626 - binary_accuracy: 0.9224 - val_loss: 0.1693 - val_binary_accuracy: 0.9634\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2523 - binary_accuracy: 0.9265 - val_loss: 0.1692 - val_binary_accuracy: 0.9634\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2620 - binary_accuracy: 0.9265 - val_loss: 0.1694 - val_binary_accuracy: 0.9634\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2635 - binary_accuracy: 0.9265 - val_loss: 0.1697 - val_binary_accuracy: 0.9634\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2697 - binary_accuracy: 0.9265 - val_loss: 0.1700 - val_binary_accuracy: 0.9634\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2672 - binary_accuracy: 0.9224 - val_loss: 0.1701 - val_binary_accuracy: 0.9634\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2636 - binary_accuracy: 0.9224 - val_loss: 0.1703 - val_binary_accuracy: 0.9634\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2529 - binary_accuracy: 0.9265 - val_loss: 0.1705 - val_binary_accuracy: 0.9634\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2676 - binary_accuracy: 0.9265 - val_loss: 0.1705 - val_binary_accuracy: 0.9634\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2595 - binary_accuracy: 0.9265 - val_loss: 0.1705 - val_binary_accuracy: 0.9634\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2590 - binary_accuracy: 0.9224 - val_loss: 0.1706 - val_binary_accuracy: 0.9634\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2629 - binary_accuracy: 0.9265 - val_loss: 0.1707 - val_binary_accuracy: 0.9634\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2692 - binary_accuracy: 0.9265 - val_loss: 0.1709 - val_binary_accuracy: 0.9634\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2530 - binary_accuracy: 0.9265 - val_loss: 0.1710 - val_binary_accuracy: 0.9634\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2595 - binary_accuracy: 0.9265 - val_loss: 0.1712 - val_binary_accuracy: 0.9634\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2563 - binary_accuracy: 0.9265 - val_loss: 0.1713 - val_binary_accuracy: 0.9634\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2547 - binary_accuracy: 0.9265 - val_loss: 0.1714 - val_binary_accuracy: 0.9634\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2667 - binary_accuracy: 0.9265 - val_loss: 0.1715 - val_binary_accuracy: 0.9634\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2510 - binary_accuracy: 0.9265 - val_loss: 0.1715 - val_binary_accuracy: 0.9634\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2742 - binary_accuracy: 0.9265 - val_loss: 0.1715 - val_binary_accuracy: 0.9634\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2696 - binary_accuracy: 0.9265 - val_loss: 0.1714 - val_binary_accuracy: 0.9634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2582 - binary_accuracy: 0.9265 - val_loss: 0.1713 - val_binary_accuracy: 0.9634\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2602 - binary_accuracy: 0.9265 - val_loss: 0.1712 - val_binary_accuracy: 0.9634\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2549 - binary_accuracy: 0.9265 - val_loss: 0.1709 - val_binary_accuracy: 0.9634\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2583 - binary_accuracy: 0.9224 - val_loss: 0.1706 - val_binary_accuracy: 0.9634\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2490 - binary_accuracy: 0.9265 - val_loss: 0.1703 - val_binary_accuracy: 0.9634\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2577 - binary_accuracy: 0.9265 - val_loss: 0.1700 - val_binary_accuracy: 0.9634\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2640 - binary_accuracy: 0.9265 - val_loss: 0.1698 - val_binary_accuracy: 0.9634\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2554 - binary_accuracy: 0.9265 - val_loss: 0.1697 - val_binary_accuracy: 0.9634\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2559 - binary_accuracy: 0.9265 - val_loss: 0.1696 - val_binary_accuracy: 0.9634\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2544 - binary_accuracy: 0.9265 - val_loss: 0.1694 - val_binary_accuracy: 0.9634\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2609 - binary_accuracy: 0.9265 - val_loss: 0.1692 - val_binary_accuracy: 0.9634\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2672 - binary_accuracy: 0.9265 - val_loss: 0.1691 - val_binary_accuracy: 0.9634\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2725 - binary_accuracy: 0.9224 - val_loss: 0.1689 - val_binary_accuracy: 0.9634\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2588 - binary_accuracy: 0.9224 - val_loss: 0.1687 - val_binary_accuracy: 0.9634\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2616 - binary_accuracy: 0.9265 - val_loss: 0.1685 - val_binary_accuracy: 0.9634\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2589 - binary_accuracy: 0.9265 - val_loss: 0.1683 - val_binary_accuracy: 0.9634\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2532 - binary_accuracy: 0.9265 - val_loss: 0.1682 - val_binary_accuracy: 0.9634\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2595 - binary_accuracy: 0.9265 - val_loss: 0.1681 - val_binary_accuracy: 0.9634\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2572 - binary_accuracy: 0.9265 - val_loss: 0.1680 - val_binary_accuracy: 0.9634\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2528 - binary_accuracy: 0.9265 - val_loss: 0.1679 - val_binary_accuracy: 0.9634\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2506 - binary_accuracy: 0.9265 - val_loss: 0.1679 - val_binary_accuracy: 0.9634\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2489 - binary_accuracy: 0.9265 - val_loss: 0.1678 - val_binary_accuracy: 0.9634\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2538 - binary_accuracy: 0.9265 - val_loss: 0.1677 - val_binary_accuracy: 0.9634\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2567 - binary_accuracy: 0.9265 - val_loss: 0.1677 - val_binary_accuracy: 0.9634\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2495 - binary_accuracy: 0.9265 - val_loss: 0.1677 - val_binary_accuracy: 0.9634\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2551 - binary_accuracy: 0.9224 - val_loss: 0.1677 - val_binary_accuracy: 0.9634\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2594 - binary_accuracy: 0.9265 - val_loss: 0.1678 - val_binary_accuracy: 0.9634\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2468 - binary_accuracy: 0.9265 - val_loss: 0.1678 - val_binary_accuracy: 0.9634\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2491 - binary_accuracy: 0.9265 - val_loss: 0.1679 - val_binary_accuracy: 0.9634\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2550 - binary_accuracy: 0.9265 - val_loss: 0.1680 - val_binary_accuracy: 0.9634\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2525 - binary_accuracy: 0.9224 - val_loss: 0.1679 - val_binary_accuracy: 0.9634\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2524 - binary_accuracy: 0.9265 - val_loss: 0.1679 - val_binary_accuracy: 0.9634\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2505 - binary_accuracy: 0.9265 - val_loss: 0.1677 - val_binary_accuracy: 0.9634\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2482 - binary_accuracy: 0.9265 - val_loss: 0.1675 - val_binary_accuracy: 0.9634\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2517 - binary_accuracy: 0.9265 - val_loss: 0.1673 - val_binary_accuracy: 0.9634\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2459 - binary_accuracy: 0.9265 - val_loss: 0.1671 - val_binary_accuracy: 0.9634\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2569 - binary_accuracy: 0.9265 - val_loss: 0.1669 - val_binary_accuracy: 0.9634\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2573 - binary_accuracy: 0.9224 - val_loss: 0.1668 - val_binary_accuracy: 0.9634\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2484 - binary_accuracy: 0.9265 - val_loss: 0.1667 - val_binary_accuracy: 0.9634\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2562 - binary_accuracy: 0.9265 - val_loss: 0.1666 - val_binary_accuracy: 0.9634\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2463 - binary_accuracy: 0.9265 - val_loss: 0.1666 - val_binary_accuracy: 0.9634\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2399 - binary_accuracy: 0.9265 - val_loss: 0.1665 - val_binary_accuracy: 0.9634\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2468 - binary_accuracy: 0.9265 - val_loss: 0.1664 - val_binary_accuracy: 0.9634\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2510 - binary_accuracy: 0.9265 - val_loss: 0.1663 - val_binary_accuracy: 0.9634\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2496 - binary_accuracy: 0.9265 - val_loss: 0.1663 - val_binary_accuracy: 0.9634\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2454 - binary_accuracy: 0.9265 - val_loss: 0.1661 - val_binary_accuracy: 0.9634\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2445 - binary_accuracy: 0.9265 - val_loss: 0.1660 - val_binary_accuracy: 0.9634\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2413 - binary_accuracy: 0.9265 - val_loss: 0.1659 - val_binary_accuracy: 0.9634\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2709 - binary_accuracy: 0.9265 - val_loss: 0.1657 - val_binary_accuracy: 0.9634\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2459 - binary_accuracy: 0.9265 - val_loss: 0.1655 - val_binary_accuracy: 0.9634\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2418 - binary_accuracy: 0.9265 - val_loss: 0.1653 - val_binary_accuracy: 0.9634\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2363 - binary_accuracy: 0.9306 - val_loss: 0.1651 - val_binary_accuracy: 0.9634\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2404 - binary_accuracy: 0.9265 - val_loss: 0.1648 - val_binary_accuracy: 0.9634\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2496 - binary_accuracy: 0.9265 - val_loss: 0.1646 - val_binary_accuracy: 0.9634\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2387 - binary_accuracy: 0.9265 - val_loss: 0.1643 - val_binary_accuracy: 0.9634\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2562 - binary_accuracy: 0.9265 - val_loss: 0.1640 - val_binary_accuracy: 0.9634\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2562 - binary_accuracy: 0.9265 - val_loss: 0.1638 - val_binary_accuracy: 0.9634\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2447 - binary_accuracy: 0.9265 - val_loss: 0.1634 - val_binary_accuracy: 0.9634\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2409 - binary_accuracy: 0.9265 - val_loss: 0.1631 - val_binary_accuracy: 0.9634\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2416 - binary_accuracy: 0.9265 - val_loss: 0.1626 - val_binary_accuracy: 0.9634\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2522 - binary_accuracy: 0.9265 - val_loss: 0.1622 - val_binary_accuracy: 0.9634\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2457 - binary_accuracy: 0.9265 - val_loss: 0.1619 - val_binary_accuracy: 0.9634\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2260 - binary_accuracy: 0.9265 - val_loss: 0.1614 - val_binary_accuracy: 0.9634\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2446 - binary_accuracy: 0.9265 - val_loss: 0.1611 - val_binary_accuracy: 0.9634\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2468 - binary_accuracy: 0.9224 - val_loss: 0.1606 - val_binary_accuracy: 0.9634\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2468 - binary_accuracy: 0.9265 - val_loss: 0.1604 - val_binary_accuracy: 0.9634\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2423 - binary_accuracy: 0.9306 - val_loss: 0.1603 - val_binary_accuracy: 0.9634\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2412 - binary_accuracy: 0.9265 - val_loss: 0.1601 - val_binary_accuracy: 0.9634\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2480 - binary_accuracy: 0.9265 - val_loss: 0.1598 - val_binary_accuracy: 0.9634\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2465 - binary_accuracy: 0.9265 - val_loss: 0.1594 - val_binary_accuracy: 0.9634\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2588 - binary_accuracy: 0.9224 - val_loss: 0.1592 - val_binary_accuracy: 0.9634\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2470 - binary_accuracy: 0.9265 - val_loss: 0.1591 - val_binary_accuracy: 0.9634\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2459 - binary_accuracy: 0.9265 - val_loss: 0.1589 - val_binary_accuracy: 0.9634\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2533 - binary_accuracy: 0.9265 - val_loss: 0.1588 - val_binary_accuracy: 0.9634\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2452 - binary_accuracy: 0.9265 - val_loss: 0.1586 - val_binary_accuracy: 0.9634\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2516 - binary_accuracy: 0.9265 - val_loss: 0.1585 - val_binary_accuracy: 0.9634\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2479 - binary_accuracy: 0.9265 - val_loss: 0.1584 - val_binary_accuracy: 0.9634\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2321 - binary_accuracy: 0.9265 - val_loss: 0.1582 - val_binary_accuracy: 0.9634\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2494 - binary_accuracy: 0.9265 - val_loss: 0.1581 - val_binary_accuracy: 0.9634\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2393 - binary_accuracy: 0.9265 - val_loss: 0.1582 - val_binary_accuracy: 0.9634\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2467 - binary_accuracy: 0.9224 - val_loss: 0.1583 - val_binary_accuracy: 0.9634\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2422 - binary_accuracy: 0.9265 - val_loss: 0.1584 - val_binary_accuracy: 0.9634\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2388 - binary_accuracy: 0.9265 - val_loss: 0.1583 - val_binary_accuracy: 0.9634\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2511 - binary_accuracy: 0.9224 - val_loss: 0.1583 - val_binary_accuracy: 0.9634\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2424 - binary_accuracy: 0.9224 - val_loss: 0.1582 - val_binary_accuracy: 0.9634\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2323 - binary_accuracy: 0.9265 - val_loss: 0.1579 - val_binary_accuracy: 0.9634\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2473 - binary_accuracy: 0.9265 - val_loss: 0.1577 - val_binary_accuracy: 0.9634\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2386 - binary_accuracy: 0.9265 - val_loss: 0.1574 - val_binary_accuracy: 0.9634\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2408 - binary_accuracy: 0.9265 - val_loss: 0.1571 - val_binary_accuracy: 0.9634\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2409 - binary_accuracy: 0.9265 - val_loss: 0.1567 - val_binary_accuracy: 0.9634\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2393 - binary_accuracy: 0.9265 - val_loss: 0.1565 - val_binary_accuracy: 0.9634\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2407 - binary_accuracy: 0.9265 - val_loss: 0.1561 - val_binary_accuracy: 0.9634\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2413 - binary_accuracy: 0.9224 - val_loss: 0.1558 - val_binary_accuracy: 0.9634\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2396 - binary_accuracy: 0.9224 - val_loss: 0.1554 - val_binary_accuracy: 0.9634\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2330 - binary_accuracy: 0.9265 - val_loss: 0.1550 - val_binary_accuracy: 0.9634\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2328 - binary_accuracy: 0.9265 - val_loss: 0.1543 - val_binary_accuracy: 0.9634\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2458 - binary_accuracy: 0.9224 - val_loss: 0.1538 - val_binary_accuracy: 0.9634\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2364 - binary_accuracy: 0.9265 - val_loss: 0.1533 - val_binary_accuracy: 0.9634\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2214 - binary_accuracy: 0.9265 - val_loss: 0.1529 - val_binary_accuracy: 0.9634\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2393 - binary_accuracy: 0.9265 - val_loss: 0.1526 - val_binary_accuracy: 0.9634\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2329 - binary_accuracy: 0.9265 - val_loss: 0.1524 - val_binary_accuracy: 0.9634\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2365 - binary_accuracy: 0.9265 - val_loss: 0.1519 - val_binary_accuracy: 0.9634\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2337 - binary_accuracy: 0.9265 - val_loss: 0.1514 - val_binary_accuracy: 0.9634\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2297 - binary_accuracy: 0.9265 - val_loss: 0.1508 - val_binary_accuracy: 0.9634\n",
      "Epoch 211/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2379 - binary_accuracy: 0.9265 - val_loss: 0.1502 - val_binary_accuracy: 0.9634\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2510 - binary_accuracy: 0.9224 - val_loss: 0.1499 - val_binary_accuracy: 0.9634\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2276 - binary_accuracy: 0.9265 - val_loss: 0.1496 - val_binary_accuracy: 0.9634\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2417 - binary_accuracy: 0.9265 - val_loss: 0.1493 - val_binary_accuracy: 0.9634\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2401 - binary_accuracy: 0.9265 - val_loss: 0.1489 - val_binary_accuracy: 0.9634\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2239 - binary_accuracy: 0.9265 - val_loss: 0.1485 - val_binary_accuracy: 0.9634\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2345 - binary_accuracy: 0.9265 - val_loss: 0.1483 - val_binary_accuracy: 0.9634\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2267 - binary_accuracy: 0.9265 - val_loss: 0.1483 - val_binary_accuracy: 0.9634\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2290 - binary_accuracy: 0.9265 - val_loss: 0.1482 - val_binary_accuracy: 0.9634\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2404 - binary_accuracy: 0.9265 - val_loss: 0.1483 - val_binary_accuracy: 0.9634\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2358 - binary_accuracy: 0.9265 - val_loss: 0.1482 - val_binary_accuracy: 0.9634\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2352 - binary_accuracy: 0.9265 - val_loss: 0.1481 - val_binary_accuracy: 0.9634\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2383 - binary_accuracy: 0.9265 - val_loss: 0.1481 - val_binary_accuracy: 0.9634\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2293 - binary_accuracy: 0.9265 - val_loss: 0.1483 - val_binary_accuracy: 0.9634\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2391 - binary_accuracy: 0.9224 - val_loss: 0.1486 - val_binary_accuracy: 0.9634\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2310 - binary_accuracy: 0.9306 - val_loss: 0.1491 - val_binary_accuracy: 0.9634\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2357 - binary_accuracy: 0.9265 - val_loss: 0.1497 - val_binary_accuracy: 0.9634\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2288 - binary_accuracy: 0.9265 - val_loss: 0.1503 - val_binary_accuracy: 0.9634\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2154 - binary_accuracy: 0.9306 - val_loss: 0.1507 - val_binary_accuracy: 0.9634\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2309 - binary_accuracy: 0.9265 - val_loss: 0.1510 - val_binary_accuracy: 0.9634\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2301 - binary_accuracy: 0.9265 - val_loss: 0.1512 - val_binary_accuracy: 0.9634\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2385 - binary_accuracy: 0.9265 - val_loss: 0.1514 - val_binary_accuracy: 0.9634\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2311 - binary_accuracy: 0.9265 - val_loss: 0.1514 - val_binary_accuracy: 0.9634\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2382 - binary_accuracy: 0.9265 - val_loss: 0.1515 - val_binary_accuracy: 0.9634\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2336 - binary_accuracy: 0.9224 - val_loss: 0.1517 - val_binary_accuracy: 0.9634\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2246 - binary_accuracy: 0.9306 - val_loss: 0.1520 - val_binary_accuracy: 0.9634\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2260 - binary_accuracy: 0.9224 - val_loss: 0.1523 - val_binary_accuracy: 0.9634\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2371 - binary_accuracy: 0.9265 - val_loss: 0.1526 - val_binary_accuracy: 0.9634\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2240 - binary_accuracy: 0.9306 - val_loss: 0.1529 - val_binary_accuracy: 0.9634\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2318 - binary_accuracy: 0.9265 - val_loss: 0.1533 - val_binary_accuracy: 0.9634\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2318 - binary_accuracy: 0.9306 - val_loss: 0.1540 - val_binary_accuracy: 0.9634\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2304 - binary_accuracy: 0.9265 - val_loss: 0.1546 - val_binary_accuracy: 0.9634\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2271 - binary_accuracy: 0.9265 - val_loss: 0.1552 - val_binary_accuracy: 0.9634\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2277 - binary_accuracy: 0.9224 - val_loss: 0.1562 - val_binary_accuracy: 0.9634\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2300 - binary_accuracy: 0.9265 - val_loss: 0.1573 - val_binary_accuracy: 0.9634\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2297 - binary_accuracy: 0.9265 - val_loss: 0.1585 - val_binary_accuracy: 0.9634\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2361 - binary_accuracy: 0.9265 - val_loss: 0.1599 - val_binary_accuracy: 0.9634\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2320 - binary_accuracy: 0.9224 - val_loss: 0.1612 - val_binary_accuracy: 0.9634\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2236 - binary_accuracy: 0.9265 - val_loss: 0.1631 - val_binary_accuracy: 0.9634\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2175 - binary_accuracy: 0.9265 - val_loss: 0.1649 - val_binary_accuracy: 0.9634\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2185 - binary_accuracy: 0.9224 - val_loss: 0.1663 - val_binary_accuracy: 0.9634\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2331 - binary_accuracy: 0.9224 - val_loss: 0.1676 - val_binary_accuracy: 0.9634\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2360 - binary_accuracy: 0.9224 - val_loss: 0.1688 - val_binary_accuracy: 0.9634\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2205 - binary_accuracy: 0.9265 - val_loss: 0.1703 - val_binary_accuracy: 0.9634\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2280 - binary_accuracy: 0.9224 - val_loss: 0.1717 - val_binary_accuracy: 0.9634\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2191 - binary_accuracy: 0.9265 - val_loss: 0.1733 - val_binary_accuracy: 0.9634\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2138 - binary_accuracy: 0.9306 - val_loss: 0.1746 - val_binary_accuracy: 0.9634\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2191 - binary_accuracy: 0.9224 - val_loss: 0.1759 - val_binary_accuracy: 0.9634\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2279 - binary_accuracy: 0.9224 - val_loss: 0.1766 - val_binary_accuracy: 0.9634\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2164 - binary_accuracy: 0.9306 - val_loss: 0.1766 - val_binary_accuracy: 0.9634\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2152 - binary_accuracy: 0.9265 - val_loss: 0.1772 - val_binary_accuracy: 0.9634\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2051 - binary_accuracy: 0.9265 - val_loss: 0.1775 - val_binary_accuracy: 0.9634\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2265 - binary_accuracy: 0.9224 - val_loss: 0.1779 - val_binary_accuracy: 0.9634\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2299 - binary_accuracy: 0.9224 - val_loss: 0.1789 - val_binary_accuracy: 0.9634\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2122 - binary_accuracy: 0.9265 - val_loss: 0.1799 - val_binary_accuracy: 0.9634\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2308 - binary_accuracy: 0.9265 - val_loss: 0.1813 - val_binary_accuracy: 0.9634\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2178 - binary_accuracy: 0.9265 - val_loss: 0.1825 - val_binary_accuracy: 0.9634\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2268 - binary_accuracy: 0.9265 - val_loss: 0.1842 - val_binary_accuracy: 0.9634\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2254 - binary_accuracy: 0.9224 - val_loss: 0.1875 - val_binary_accuracy: 0.9634\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2194 - binary_accuracy: 0.9265 - val_loss: 0.1915 - val_binary_accuracy: 0.9634\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2069 - binary_accuracy: 0.9306 - val_loss: 0.1939 - val_binary_accuracy: 0.9634\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2192 - binary_accuracy: 0.9265 - val_loss: 0.1958 - val_binary_accuracy: 0.9634\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2432 - binary_accuracy: 0.9224 - val_loss: 0.1978 - val_binary_accuracy: 0.9634\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2037 - binary_accuracy: 0.9265 - val_loss: 0.2000 - val_binary_accuracy: 0.9634\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2271 - binary_accuracy: 0.9265 - val_loss: 0.2028 - val_binary_accuracy: 0.9634\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2293 - binary_accuracy: 0.9224 - val_loss: 0.2049 - val_binary_accuracy: 0.9634\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2291 - binary_accuracy: 0.9265 - val_loss: 0.2073 - val_binary_accuracy: 0.9634\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2051 - binary_accuracy: 0.9306 - val_loss: 0.2091 - val_binary_accuracy: 0.9634\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2240 - binary_accuracy: 0.9265 - val_loss: 0.2095 - val_binary_accuracy: 0.9634\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2266 - binary_accuracy: 0.9224 - val_loss: 0.2102 - val_binary_accuracy: 0.9634\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2165 - binary_accuracy: 0.9265 - val_loss: 0.2106 - val_binary_accuracy: 0.9634\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2038 - binary_accuracy: 0.9306 - val_loss: 0.2112 - val_binary_accuracy: 0.9634\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2302 - binary_accuracy: 0.9224 - val_loss: 0.2115 - val_binary_accuracy: 0.9634\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2180 - binary_accuracy: 0.9224 - val_loss: 0.2125 - val_binary_accuracy: 0.9634\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2313 - binary_accuracy: 0.9224 - val_loss: 0.2118 - val_binary_accuracy: 0.9634\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2161 - binary_accuracy: 0.9265 - val_loss: 0.2110 - val_binary_accuracy: 0.9634\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2089 - binary_accuracy: 0.9265 - val_loss: 0.2106 - val_binary_accuracy: 0.9634\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2150 - binary_accuracy: 0.9306 - val_loss: 0.2089 - val_binary_accuracy: 0.9634\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2261 - binary_accuracy: 0.9224 - val_loss: 0.2072 - val_binary_accuracy: 0.9634\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2083 - binary_accuracy: 0.9265 - val_loss: 0.2062 - val_binary_accuracy: 0.9634\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2082 - binary_accuracy: 0.9306 - val_loss: 0.2056 - val_binary_accuracy: 0.9634\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2040 - binary_accuracy: 0.9265 - val_loss: 0.2055 - val_binary_accuracy: 0.9634\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2242 - binary_accuracy: 0.9224 - val_loss: 0.2058 - val_binary_accuracy: 0.9634\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2127 - binary_accuracy: 0.9265 - val_loss: 0.2068 - val_binary_accuracy: 0.9634\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2044 - binary_accuracy: 0.9265 - val_loss: 0.2085 - val_binary_accuracy: 0.9634\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2123 - binary_accuracy: 0.9224 - val_loss: 0.2106 - val_binary_accuracy: 0.9634\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2071 - binary_accuracy: 0.9265 - val_loss: 0.2128 - val_binary_accuracy: 0.9634\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2216 - binary_accuracy: 0.9265 - val_loss: 0.2149 - val_binary_accuracy: 0.9634\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2168 - binary_accuracy: 0.9224 - val_loss: 0.2173 - val_binary_accuracy: 0.9634\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2078 - binary_accuracy: 0.9265 - val_loss: 0.2196 - val_binary_accuracy: 0.9634\n"
     ]
    }
   ],
   "source": [
    "#Neural network with 4 hidden layers, batch normalization, and dropout\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(input_shape = [9]),\n",
    "    layers.Dense(10, activation='relu', input_shape=[9]),  \n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='relu'), \n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='relu'), \n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='relu'), \n",
    "    layers.Dropout(0.5),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "#compiling using adam optimizer\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = opt,\n",
    "    loss ='binary_crossentropy',\n",
    "    metrics =['binary_accuracy'],\n",
    ")\n",
    "\n",
    "#early stopping\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=512,\n",
    "    epochs=300,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d47cfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that I trained the neural network on\n",
    "predict_cols = ['FP', 'SB', 'BB', 'H',  'HR', 'ERA', 'R', 'X2B', 'X3B']\n",
    "# same as above, but with important information like team, year, and whether the National League was won.\n",
    "all_relevant_cols = ['FP', 'SB', 'BB', 'H', 'HR', 'ERA', 'R','X2B', 'X3B', 'teamID', 'yearID',  'LgWin']\n",
    "Z = data[(data.yearID >= 2016) & (data.yearID <= 2020) & (data.lgID == 'NL')]\n",
    "predict_these = Z[all_relevant_cols]\n",
    "predict_these_inputs = Z[predict_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6dabeb9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting model predictions from numpy array to dataframe\n",
    "predictions = pd.DataFrame(model.predict(predict_these_inputs))\n",
    "\n",
    "#Attaching new index to predict_these and predictions starting at 1 and going to 75\n",
    "new_index = [i for i in range(1, 76)]\n",
    "\n",
    "predict_these['new_index'] = new_index\n",
    "predictions['new_index'] = new_index\n",
    "predict_these.set_index('new_index', inplace = True)\n",
    "predictions.set_index('new_index', inplace = True)\n",
    "\n",
    "#combining the predictions with the data used for the predictions\n",
    "combined = pd.concat((predict_these, predictions), axis = 1)\n",
    "\n",
    "#rename column with predictions to probability_Lg_Win, display all 75 rows (5 years * 15 teams)\n",
    "\n",
    "combined.rename(columns = {0: \"probability_Lg_Win\"}, inplace=True)\n",
    "pd.set_option(\"display.max_rows\", None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5330133e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FP</th>\n",
       "      <th>SB</th>\n",
       "      <th>BB</th>\n",
       "      <th>H</th>\n",
       "      <th>HR</th>\n",
       "      <th>ERA</th>\n",
       "      <th>R</th>\n",
       "      <th>X2B</th>\n",
       "      <th>X3B</th>\n",
       "      <th>teamID</th>\n",
       "      <th>yearID</th>\n",
       "      <th>LgWin</th>\n",
       "      <th>probability_Lg_Win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.983</td>\n",
       "      <td>137</td>\n",
       "      <td>463</td>\n",
       "      <td>1479</td>\n",
       "      <td>190</td>\n",
       "      <td>5.09</td>\n",
       "      <td>752</td>\n",
       "      <td>285</td>\n",
       "      <td>56</td>\n",
       "      <td>ARI</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>2.062917e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.983</td>\n",
       "      <td>75</td>\n",
       "      <td>502</td>\n",
       "      <td>1404</td>\n",
       "      <td>122</td>\n",
       "      <td>4.51</td>\n",
       "      <td>649</td>\n",
       "      <td>295</td>\n",
       "      <td>27</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>1.684725e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983</td>\n",
       "      <td>66</td>\n",
       "      <td>656</td>\n",
       "      <td>1409</td>\n",
       "      <td>199</td>\n",
       "      <td>3.15</td>\n",
       "      <td>808</td>\n",
       "      <td>293</td>\n",
       "      <td>30</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2016</td>\n",
       "      <td>Y</td>\n",
       "      <td>9.472162e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.983</td>\n",
       "      <td>139</td>\n",
       "      <td>452</td>\n",
       "      <td>1403</td>\n",
       "      <td>164</td>\n",
       "      <td>4.91</td>\n",
       "      <td>716</td>\n",
       "      <td>277</td>\n",
       "      <td>33</td>\n",
       "      <td>CIN</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>6.352732e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.982</td>\n",
       "      <td>66</td>\n",
       "      <td>494</td>\n",
       "      <td>1544</td>\n",
       "      <td>204</td>\n",
       "      <td>4.91</td>\n",
       "      <td>845</td>\n",
       "      <td>318</td>\n",
       "      <td>47</td>\n",
       "      <td>COL</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>7.762760e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.986</td>\n",
       "      <td>45</td>\n",
       "      <td>525</td>\n",
       "      <td>1376</td>\n",
       "      <td>189</td>\n",
       "      <td>3.70</td>\n",
       "      <td>725</td>\n",
       "      <td>272</td>\n",
       "      <td>21</td>\n",
       "      <td>LAN</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>4.712927e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.985</td>\n",
       "      <td>71</td>\n",
       "      <td>447</td>\n",
       "      <td>1460</td>\n",
       "      <td>128</td>\n",
       "      <td>4.05</td>\n",
       "      <td>655</td>\n",
       "      <td>259</td>\n",
       "      <td>42</td>\n",
       "      <td>MIA</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>4.623801e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.978</td>\n",
       "      <td>181</td>\n",
       "      <td>599</td>\n",
       "      <td>1299</td>\n",
       "      <td>194</td>\n",
       "      <td>4.08</td>\n",
       "      <td>671</td>\n",
       "      <td>249</td>\n",
       "      <td>19</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>3.851510e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.985</td>\n",
       "      <td>42</td>\n",
       "      <td>517</td>\n",
       "      <td>1342</td>\n",
       "      <td>218</td>\n",
       "      <td>3.57</td>\n",
       "      <td>671</td>\n",
       "      <td>240</td>\n",
       "      <td>19</td>\n",
       "      <td>NYN</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>4.791355e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.984</td>\n",
       "      <td>96</td>\n",
       "      <td>424</td>\n",
       "      <td>1305</td>\n",
       "      <td>161</td>\n",
       "      <td>4.63</td>\n",
       "      <td>610</td>\n",
       "      <td>231</td>\n",
       "      <td>35</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>1.376867e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.982</td>\n",
       "      <td>110</td>\n",
       "      <td>561</td>\n",
       "      <td>1426</td>\n",
       "      <td>153</td>\n",
       "      <td>4.21</td>\n",
       "      <td>729</td>\n",
       "      <td>277</td>\n",
       "      <td>32</td>\n",
       "      <td>PIT</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>1.202732e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.982</td>\n",
       "      <td>125</td>\n",
       "      <td>449</td>\n",
       "      <td>1275</td>\n",
       "      <td>177</td>\n",
       "      <td>4.43</td>\n",
       "      <td>686</td>\n",
       "      <td>257</td>\n",
       "      <td>26</td>\n",
       "      <td>SDN</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>3.642738e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.988</td>\n",
       "      <td>79</td>\n",
       "      <td>572</td>\n",
       "      <td>1437</td>\n",
       "      <td>130</td>\n",
       "      <td>3.65</td>\n",
       "      <td>715</td>\n",
       "      <td>280</td>\n",
       "      <td>54</td>\n",
       "      <td>SFN</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>4.005748e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.983</td>\n",
       "      <td>35</td>\n",
       "      <td>526</td>\n",
       "      <td>1415</td>\n",
       "      <td>225</td>\n",
       "      <td>4.08</td>\n",
       "      <td>779</td>\n",
       "      <td>299</td>\n",
       "      <td>32</td>\n",
       "      <td>SLN</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>2.690497e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.988</td>\n",
       "      <td>121</td>\n",
       "      <td>536</td>\n",
       "      <td>1403</td>\n",
       "      <td>203</td>\n",
       "      <td>3.51</td>\n",
       "      <td>763</td>\n",
       "      <td>268</td>\n",
       "      <td>29</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2016</td>\n",
       "      <td>N</td>\n",
       "      <td>6.685922e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.982</td>\n",
       "      <td>103</td>\n",
       "      <td>578</td>\n",
       "      <td>1405</td>\n",
       "      <td>220</td>\n",
       "      <td>3.66</td>\n",
       "      <td>812</td>\n",
       "      <td>314</td>\n",
       "      <td>39</td>\n",
       "      <td>ARI</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>7.852897e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.984</td>\n",
       "      <td>77</td>\n",
       "      <td>474</td>\n",
       "      <td>1467</td>\n",
       "      <td>165</td>\n",
       "      <td>4.72</td>\n",
       "      <td>732</td>\n",
       "      <td>289</td>\n",
       "      <td>26</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>8.409917e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.984</td>\n",
       "      <td>62</td>\n",
       "      <td>622</td>\n",
       "      <td>1402</td>\n",
       "      <td>223</td>\n",
       "      <td>3.95</td>\n",
       "      <td>822</td>\n",
       "      <td>274</td>\n",
       "      <td>29</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>5.934635e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.986</td>\n",
       "      <td>120</td>\n",
       "      <td>565</td>\n",
       "      <td>1390</td>\n",
       "      <td>219</td>\n",
       "      <td>5.17</td>\n",
       "      <td>753</td>\n",
       "      <td>249</td>\n",
       "      <td>38</td>\n",
       "      <td>CIN</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>1.214681e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.987</td>\n",
       "      <td>59</td>\n",
       "      <td>519</td>\n",
       "      <td>1510</td>\n",
       "      <td>192</td>\n",
       "      <td>4.51</td>\n",
       "      <td>824</td>\n",
       "      <td>293</td>\n",
       "      <td>38</td>\n",
       "      <td>COL</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>1.099750e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.985</td>\n",
       "      <td>77</td>\n",
       "      <td>649</td>\n",
       "      <td>1347</td>\n",
       "      <td>221</td>\n",
       "      <td>3.38</td>\n",
       "      <td>770</td>\n",
       "      <td>312</td>\n",
       "      <td>20</td>\n",
       "      <td>LAN</td>\n",
       "      <td>2017</td>\n",
       "      <td>Y</td>\n",
       "      <td>8.584717e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.988</td>\n",
       "      <td>91</td>\n",
       "      <td>486</td>\n",
       "      <td>1497</td>\n",
       "      <td>194</td>\n",
       "      <td>4.82</td>\n",
       "      <td>778</td>\n",
       "      <td>271</td>\n",
       "      <td>31</td>\n",
       "      <td>MIA</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>9.543002e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.981</td>\n",
       "      <td>128</td>\n",
       "      <td>547</td>\n",
       "      <td>1363</td>\n",
       "      <td>224</td>\n",
       "      <td>4.00</td>\n",
       "      <td>732</td>\n",
       "      <td>267</td>\n",
       "      <td>22</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>4.266471e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.984</td>\n",
       "      <td>58</td>\n",
       "      <td>529</td>\n",
       "      <td>1379</td>\n",
       "      <td>224</td>\n",
       "      <td>5.01</td>\n",
       "      <td>735</td>\n",
       "      <td>286</td>\n",
       "      <td>28</td>\n",
       "      <td>NYN</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>6.383359e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.986</td>\n",
       "      <td>59</td>\n",
       "      <td>494</td>\n",
       "      <td>1382</td>\n",
       "      <td>174</td>\n",
       "      <td>4.55</td>\n",
       "      <td>690</td>\n",
       "      <td>287</td>\n",
       "      <td>36</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>2.067953e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.984</td>\n",
       "      <td>67</td>\n",
       "      <td>519</td>\n",
       "      <td>1331</td>\n",
       "      <td>151</td>\n",
       "      <td>4.22</td>\n",
       "      <td>668</td>\n",
       "      <td>249</td>\n",
       "      <td>36</td>\n",
       "      <td>PIT</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>2.001047e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.981</td>\n",
       "      <td>89</td>\n",
       "      <td>460</td>\n",
       "      <td>1251</td>\n",
       "      <td>189</td>\n",
       "      <td>4.67</td>\n",
       "      <td>604</td>\n",
       "      <td>227</td>\n",
       "      <td>31</td>\n",
       "      <td>SDN</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>1.160722e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.985</td>\n",
       "      <td>76</td>\n",
       "      <td>467</td>\n",
       "      <td>1382</td>\n",
       "      <td>128</td>\n",
       "      <td>4.50</td>\n",
       "      <td>639</td>\n",
       "      <td>290</td>\n",
       "      <td>28</td>\n",
       "      <td>SFN</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>2.204180e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.984</td>\n",
       "      <td>81</td>\n",
       "      <td>593</td>\n",
       "      <td>1402</td>\n",
       "      <td>196</td>\n",
       "      <td>4.01</td>\n",
       "      <td>761</td>\n",
       "      <td>284</td>\n",
       "      <td>28</td>\n",
       "      <td>SLN</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>1.555201e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.985</td>\n",
       "      <td>108</td>\n",
       "      <td>542</td>\n",
       "      <td>1477</td>\n",
       "      <td>215</td>\n",
       "      <td>3.88</td>\n",
       "      <td>819</td>\n",
       "      <td>311</td>\n",
       "      <td>31</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2017</td>\n",
       "      <td>N</td>\n",
       "      <td>5.940452e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.988</td>\n",
       "      <td>79</td>\n",
       "      <td>560</td>\n",
       "      <td>1283</td>\n",
       "      <td>176</td>\n",
       "      <td>3.72</td>\n",
       "      <td>693</td>\n",
       "      <td>259</td>\n",
       "      <td>50</td>\n",
       "      <td>ARI</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>6.932616e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.986</td>\n",
       "      <td>90</td>\n",
       "      <td>511</td>\n",
       "      <td>1433</td>\n",
       "      <td>175</td>\n",
       "      <td>3.75</td>\n",
       "      <td>759</td>\n",
       "      <td>314</td>\n",
       "      <td>29</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>3.586933e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.983</td>\n",
       "      <td>66</td>\n",
       "      <td>576</td>\n",
       "      <td>1453</td>\n",
       "      <td>167</td>\n",
       "      <td>3.65</td>\n",
       "      <td>761</td>\n",
       "      <td>286</td>\n",
       "      <td>34</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>5.702186e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.984</td>\n",
       "      <td>77</td>\n",
       "      <td>559</td>\n",
       "      <td>1404</td>\n",
       "      <td>172</td>\n",
       "      <td>4.63</td>\n",
       "      <td>696</td>\n",
       "      <td>251</td>\n",
       "      <td>25</td>\n",
       "      <td>CIN</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>2.646148e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.988</td>\n",
       "      <td>95</td>\n",
       "      <td>507</td>\n",
       "      <td>1418</td>\n",
       "      <td>210</td>\n",
       "      <td>4.33</td>\n",
       "      <td>780</td>\n",
       "      <td>280</td>\n",
       "      <td>42</td>\n",
       "      <td>COL</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>4.773617e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.983</td>\n",
       "      <td>75</td>\n",
       "      <td>647</td>\n",
       "      <td>1394</td>\n",
       "      <td>235</td>\n",
       "      <td>3.38</td>\n",
       "      <td>804</td>\n",
       "      <td>296</td>\n",
       "      <td>33</td>\n",
       "      <td>LAN</td>\n",
       "      <td>2018</td>\n",
       "      <td>Y</td>\n",
       "      <td>9.205669e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.986</td>\n",
       "      <td>45</td>\n",
       "      <td>455</td>\n",
       "      <td>1303</td>\n",
       "      <td>128</td>\n",
       "      <td>4.76</td>\n",
       "      <td>589</td>\n",
       "      <td>222</td>\n",
       "      <td>24</td>\n",
       "      <td>MIA</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>6.398028e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.982</td>\n",
       "      <td>124</td>\n",
       "      <td>537</td>\n",
       "      <td>1398</td>\n",
       "      <td>218</td>\n",
       "      <td>3.73</td>\n",
       "      <td>754</td>\n",
       "      <td>252</td>\n",
       "      <td>24</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>2.756757e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.985</td>\n",
       "      <td>71</td>\n",
       "      <td>566</td>\n",
       "      <td>1282</td>\n",
       "      <td>170</td>\n",
       "      <td>4.07</td>\n",
       "      <td>676</td>\n",
       "      <td>265</td>\n",
       "      <td>34</td>\n",
       "      <td>NYN</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>1.796544e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.979</td>\n",
       "      <td>69</td>\n",
       "      <td>582</td>\n",
       "      <td>1270</td>\n",
       "      <td>186</td>\n",
       "      <td>4.14</td>\n",
       "      <td>677</td>\n",
       "      <td>241</td>\n",
       "      <td>30</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>1.342475e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.982</td>\n",
       "      <td>70</td>\n",
       "      <td>474</td>\n",
       "      <td>1381</td>\n",
       "      <td>157</td>\n",
       "      <td>4.00</td>\n",
       "      <td>692</td>\n",
       "      <td>290</td>\n",
       "      <td>38</td>\n",
       "      <td>PIT</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>9.030312e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.983</td>\n",
       "      <td>95</td>\n",
       "      <td>471</td>\n",
       "      <td>1289</td>\n",
       "      <td>162</td>\n",
       "      <td>4.40</td>\n",
       "      <td>617</td>\n",
       "      <td>250</td>\n",
       "      <td>30</td>\n",
       "      <td>SDN</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>2.129972e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.984</td>\n",
       "      <td>77</td>\n",
       "      <td>448</td>\n",
       "      <td>1324</td>\n",
       "      <td>133</td>\n",
       "      <td>3.95</td>\n",
       "      <td>603</td>\n",
       "      <td>255</td>\n",
       "      <td>30</td>\n",
       "      <td>SFN</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>1.053423e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.978</td>\n",
       "      <td>63</td>\n",
       "      <td>525</td>\n",
       "      <td>1369</td>\n",
       "      <td>205</td>\n",
       "      <td>3.85</td>\n",
       "      <td>759</td>\n",
       "      <td>248</td>\n",
       "      <td>9</td>\n",
       "      <td>SLN</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>4.696625e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.989</td>\n",
       "      <td>119</td>\n",
       "      <td>631</td>\n",
       "      <td>1402</td>\n",
       "      <td>191</td>\n",
       "      <td>4.04</td>\n",
       "      <td>771</td>\n",
       "      <td>284</td>\n",
       "      <td>25</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2018</td>\n",
       "      <td>N</td>\n",
       "      <td>4.727721e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.986</td>\n",
       "      <td>88</td>\n",
       "      <td>540</td>\n",
       "      <td>1419</td>\n",
       "      <td>220</td>\n",
       "      <td>4.25</td>\n",
       "      <td>813</td>\n",
       "      <td>288</td>\n",
       "      <td>40</td>\n",
       "      <td>ARI</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>1.088586e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.987</td>\n",
       "      <td>89</td>\n",
       "      <td>619</td>\n",
       "      <td>1432</td>\n",
       "      <td>249</td>\n",
       "      <td>4.19</td>\n",
       "      <td>855</td>\n",
       "      <td>277</td>\n",
       "      <td>29</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>2.264199e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.981</td>\n",
       "      <td>45</td>\n",
       "      <td>581</td>\n",
       "      <td>1378</td>\n",
       "      <td>256</td>\n",
       "      <td>4.10</td>\n",
       "      <td>814</td>\n",
       "      <td>270</td>\n",
       "      <td>26</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>3.066266e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.984</td>\n",
       "      <td>80</td>\n",
       "      <td>492</td>\n",
       "      <td>1328</td>\n",
       "      <td>227</td>\n",
       "      <td>4.18</td>\n",
       "      <td>701</td>\n",
       "      <td>235</td>\n",
       "      <td>27</td>\n",
       "      <td>CIN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>3.302515e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.984</td>\n",
       "      <td>71</td>\n",
       "      <td>489</td>\n",
       "      <td>1502</td>\n",
       "      <td>224</td>\n",
       "      <td>5.56</td>\n",
       "      <td>835</td>\n",
       "      <td>323</td>\n",
       "      <td>41</td>\n",
       "      <td>COL</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>1.111597e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.982</td>\n",
       "      <td>57</td>\n",
       "      <td>607</td>\n",
       "      <td>1414</td>\n",
       "      <td>279</td>\n",
       "      <td>3.37</td>\n",
       "      <td>886</td>\n",
       "      <td>302</td>\n",
       "      <td>20</td>\n",
       "      <td>LAN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>9.841037e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.984</td>\n",
       "      <td>55</td>\n",
       "      <td>395</td>\n",
       "      <td>1326</td>\n",
       "      <td>146</td>\n",
       "      <td>4.74</td>\n",
       "      <td>615</td>\n",
       "      <td>265</td>\n",
       "      <td>18</td>\n",
       "      <td>MIA</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>1.108356e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.983</td>\n",
       "      <td>101</td>\n",
       "      <td>629</td>\n",
       "      <td>1366</td>\n",
       "      <td>250</td>\n",
       "      <td>4.40</td>\n",
       "      <td>769</td>\n",
       "      <td>279</td>\n",
       "      <td>17</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>4.216343e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.983</td>\n",
       "      <td>56</td>\n",
       "      <td>516</td>\n",
       "      <td>1445</td>\n",
       "      <td>242</td>\n",
       "      <td>4.24</td>\n",
       "      <td>791</td>\n",
       "      <td>280</td>\n",
       "      <td>17</td>\n",
       "      <td>NYN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>2.735195e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.984</td>\n",
       "      <td>78</td>\n",
       "      <td>562</td>\n",
       "      <td>1369</td>\n",
       "      <td>215</td>\n",
       "      <td>4.53</td>\n",
       "      <td>774</td>\n",
       "      <td>311</td>\n",
       "      <td>26</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>4.606992e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.980</td>\n",
       "      <td>64</td>\n",
       "      <td>425</td>\n",
       "      <td>1497</td>\n",
       "      <td>163</td>\n",
       "      <td>5.18</td>\n",
       "      <td>758</td>\n",
       "      <td>315</td>\n",
       "      <td>38</td>\n",
       "      <td>PIT</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>1.841962e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.980</td>\n",
       "      <td>70</td>\n",
       "      <td>504</td>\n",
       "      <td>1281</td>\n",
       "      <td>219</td>\n",
       "      <td>4.60</td>\n",
       "      <td>682</td>\n",
       "      <td>224</td>\n",
       "      <td>24</td>\n",
       "      <td>SDN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>7.430613e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.985</td>\n",
       "      <td>47</td>\n",
       "      <td>475</td>\n",
       "      <td>1332</td>\n",
       "      <td>167</td>\n",
       "      <td>4.38</td>\n",
       "      <td>678</td>\n",
       "      <td>300</td>\n",
       "      <td>26</td>\n",
       "      <td>SFN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>1.969606e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.989</td>\n",
       "      <td>116</td>\n",
       "      <td>561</td>\n",
       "      <td>1336</td>\n",
       "      <td>210</td>\n",
       "      <td>3.80</td>\n",
       "      <td>764</td>\n",
       "      <td>246</td>\n",
       "      <td>24</td>\n",
       "      <td>SLN</td>\n",
       "      <td>2019</td>\n",
       "      <td>N</td>\n",
       "      <td>3.292796e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.985</td>\n",
       "      <td>116</td>\n",
       "      <td>584</td>\n",
       "      <td>1460</td>\n",
       "      <td>231</td>\n",
       "      <td>4.27</td>\n",
       "      <td>873</td>\n",
       "      <td>298</td>\n",
       "      <td>27</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2019</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.166259e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.983</td>\n",
       "      <td>23</td>\n",
       "      <td>181</td>\n",
       "      <td>482</td>\n",
       "      <td>58</td>\n",
       "      <td>4.84</td>\n",
       "      <td>269</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>ARI</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.797345e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.985</td>\n",
       "      <td>23</td>\n",
       "      <td>239</td>\n",
       "      <td>556</td>\n",
       "      <td>103</td>\n",
       "      <td>4.41</td>\n",
       "      <td>348</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>2.289763e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.986</td>\n",
       "      <td>24</td>\n",
       "      <td>229</td>\n",
       "      <td>422</td>\n",
       "      <td>74</td>\n",
       "      <td>3.99</td>\n",
       "      <td>265</td>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.454528e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.986</td>\n",
       "      <td>28</td>\n",
       "      <td>239</td>\n",
       "      <td>390</td>\n",
       "      <td>90</td>\n",
       "      <td>3.84</td>\n",
       "      <td>243</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>CIN</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.679467e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.981</td>\n",
       "      <td>42</td>\n",
       "      <td>161</td>\n",
       "      <td>528</td>\n",
       "      <td>63</td>\n",
       "      <td>5.59</td>\n",
       "      <td>275</td>\n",
       "      <td>84</td>\n",
       "      <td>16</td>\n",
       "      <td>COL</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.921992e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.982</td>\n",
       "      <td>29</td>\n",
       "      <td>228</td>\n",
       "      <td>523</td>\n",
       "      <td>118</td>\n",
       "      <td>3.02</td>\n",
       "      <td>349</td>\n",
       "      <td>97</td>\n",
       "      <td>6</td>\n",
       "      <td>LAN</td>\n",
       "      <td>2020</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.891421e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.981</td>\n",
       "      <td>51</td>\n",
       "      <td>191</td>\n",
       "      <td>472</td>\n",
       "      <td>60</td>\n",
       "      <td>4.86</td>\n",
       "      <td>263</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>MIA</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>7.790432e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.984</td>\n",
       "      <td>15</td>\n",
       "      <td>221</td>\n",
       "      <td>429</td>\n",
       "      <td>75</td>\n",
       "      <td>4.16</td>\n",
       "      <td>247</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>MIL</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>8.181820e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.985</td>\n",
       "      <td>20</td>\n",
       "      <td>197</td>\n",
       "      <td>551</td>\n",
       "      <td>86</td>\n",
       "      <td>4.98</td>\n",
       "      <td>286</td>\n",
       "      <td>106</td>\n",
       "      <td>7</td>\n",
       "      <td>NYN</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>2.679697e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.983</td>\n",
       "      <td>35</td>\n",
       "      <td>229</td>\n",
       "      <td>500</td>\n",
       "      <td>82</td>\n",
       "      <td>5.14</td>\n",
       "      <td>306</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.332976e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.978</td>\n",
       "      <td>16</td>\n",
       "      <td>167</td>\n",
       "      <td>425</td>\n",
       "      <td>59</td>\n",
       "      <td>4.68</td>\n",
       "      <td>219</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>PIT</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.515237e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.985</td>\n",
       "      <td>55</td>\n",
       "      <td>204</td>\n",
       "      <td>506</td>\n",
       "      <td>95</td>\n",
       "      <td>3.86</td>\n",
       "      <td>325</td>\n",
       "      <td>103</td>\n",
       "      <td>12</td>\n",
       "      <td>SDN</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>8.795061e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.980</td>\n",
       "      <td>19</td>\n",
       "      <td>195</td>\n",
       "      <td>532</td>\n",
       "      <td>81</td>\n",
       "      <td>4.64</td>\n",
       "      <td>299</td>\n",
       "      <td>107</td>\n",
       "      <td>14</td>\n",
       "      <td>SFN</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>7.742470e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.983</td>\n",
       "      <td>18</td>\n",
       "      <td>205</td>\n",
       "      <td>410</td>\n",
       "      <td>51</td>\n",
       "      <td>3.90</td>\n",
       "      <td>240</td>\n",
       "      <td>73</td>\n",
       "      <td>7</td>\n",
       "      <td>SLN</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.235227e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.981</td>\n",
       "      <td>33</td>\n",
       "      <td>192</td>\n",
       "      <td>519</td>\n",
       "      <td>66</td>\n",
       "      <td>5.09</td>\n",
       "      <td>293</td>\n",
       "      <td>112</td>\n",
       "      <td>12</td>\n",
       "      <td>WAS</td>\n",
       "      <td>2020</td>\n",
       "      <td>N</td>\n",
       "      <td>1.272267e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              FP   SB   BB     H   HR   ERA    R  X2B  X3B teamID  yearID  \\\n",
       "new_index                                                                   \n",
       "1          0.983  137  463  1479  190  5.09  752  285   56    ARI    2016   \n",
       "2          0.983   75  502  1404  122  4.51  649  295   27    ATL    2016   \n",
       "3          0.983   66  656  1409  199  3.15  808  293   30    CHN    2016   \n",
       "4          0.983  139  452  1403  164  4.91  716  277   33    CIN    2016   \n",
       "5          0.982   66  494  1544  204  4.91  845  318   47    COL    2016   \n",
       "6          0.986   45  525  1376  189  3.70  725  272   21    LAN    2016   \n",
       "7          0.985   71  447  1460  128  4.05  655  259   42    MIA    2016   \n",
       "8          0.978  181  599  1299  194  4.08  671  249   19    MIL    2016   \n",
       "9          0.985   42  517  1342  218  3.57  671  240   19    NYN    2016   \n",
       "10         0.984   96  424  1305  161  4.63  610  231   35    PHI    2016   \n",
       "11         0.982  110  561  1426  153  4.21  729  277   32    PIT    2016   \n",
       "12         0.982  125  449  1275  177  4.43  686  257   26    SDN    2016   \n",
       "13         0.988   79  572  1437  130  3.65  715  280   54    SFN    2016   \n",
       "14         0.983   35  526  1415  225  4.08  779  299   32    SLN    2016   \n",
       "15         0.988  121  536  1403  203  3.51  763  268   29    WAS    2016   \n",
       "16         0.982  103  578  1405  220  3.66  812  314   39    ARI    2017   \n",
       "17         0.984   77  474  1467  165  4.72  732  289   26    ATL    2017   \n",
       "18         0.984   62  622  1402  223  3.95  822  274   29    CHN    2017   \n",
       "19         0.986  120  565  1390  219  5.17  753  249   38    CIN    2017   \n",
       "20         0.987   59  519  1510  192  4.51  824  293   38    COL    2017   \n",
       "21         0.985   77  649  1347  221  3.38  770  312   20    LAN    2017   \n",
       "22         0.988   91  486  1497  194  4.82  778  271   31    MIA    2017   \n",
       "23         0.981  128  547  1363  224  4.00  732  267   22    MIL    2017   \n",
       "24         0.984   58  529  1379  224  5.01  735  286   28    NYN    2017   \n",
       "25         0.986   59  494  1382  174  4.55  690  287   36    PHI    2017   \n",
       "26         0.984   67  519  1331  151  4.22  668  249   36    PIT    2017   \n",
       "27         0.981   89  460  1251  189  4.67  604  227   31    SDN    2017   \n",
       "28         0.985   76  467  1382  128  4.50  639  290   28    SFN    2017   \n",
       "29         0.984   81  593  1402  196  4.01  761  284   28    SLN    2017   \n",
       "30         0.985  108  542  1477  215  3.88  819  311   31    WAS    2017   \n",
       "31         0.988   79  560  1283  176  3.72  693  259   50    ARI    2018   \n",
       "32         0.986   90  511  1433  175  3.75  759  314   29    ATL    2018   \n",
       "33         0.983   66  576  1453  167  3.65  761  286   34    CHN    2018   \n",
       "34         0.984   77  559  1404  172  4.63  696  251   25    CIN    2018   \n",
       "35         0.988   95  507  1418  210  4.33  780  280   42    COL    2018   \n",
       "36         0.983   75  647  1394  235  3.38  804  296   33    LAN    2018   \n",
       "37         0.986   45  455  1303  128  4.76  589  222   24    MIA    2018   \n",
       "38         0.982  124  537  1398  218  3.73  754  252   24    MIL    2018   \n",
       "39         0.985   71  566  1282  170  4.07  676  265   34    NYN    2018   \n",
       "40         0.979   69  582  1270  186  4.14  677  241   30    PHI    2018   \n",
       "41         0.982   70  474  1381  157  4.00  692  290   38    PIT    2018   \n",
       "42         0.983   95  471  1289  162  4.40  617  250   30    SDN    2018   \n",
       "43         0.984   77  448  1324  133  3.95  603  255   30    SFN    2018   \n",
       "44         0.978   63  525  1369  205  3.85  759  248    9    SLN    2018   \n",
       "45         0.989  119  631  1402  191  4.04  771  284   25    WAS    2018   \n",
       "46         0.986   88  540  1419  220  4.25  813  288   40    ARI    2019   \n",
       "47         0.987   89  619  1432  249  4.19  855  277   29    ATL    2019   \n",
       "48         0.981   45  581  1378  256  4.10  814  270   26    CHN    2019   \n",
       "49         0.984   80  492  1328  227  4.18  701  235   27    CIN    2019   \n",
       "50         0.984   71  489  1502  224  5.56  835  323   41    COL    2019   \n",
       "51         0.982   57  607  1414  279  3.37  886  302   20    LAN    2019   \n",
       "52         0.984   55  395  1326  146  4.74  615  265   18    MIA    2019   \n",
       "53         0.983  101  629  1366  250  4.40  769  279   17    MIL    2019   \n",
       "54         0.983   56  516  1445  242  4.24  791  280   17    NYN    2019   \n",
       "55         0.984   78  562  1369  215  4.53  774  311   26    PHI    2019   \n",
       "56         0.980   64  425  1497  163  5.18  758  315   38    PIT    2019   \n",
       "57         0.980   70  504  1281  219  4.60  682  224   24    SDN    2019   \n",
       "58         0.985   47  475  1332  167  4.38  678  300   26    SFN    2019   \n",
       "59         0.989  116  561  1336  210  3.80  764  246   24    SLN    2019   \n",
       "60         0.985  116  584  1460  231  4.27  873  298   27    WAS    2019   \n",
       "61         0.983   23  181   482   58  4.84  269  101   12    ARI    2020   \n",
       "62         0.985   23  239   556  103  4.41  348  130    3    ATL    2020   \n",
       "63         0.986   24  229   422   74  3.99  265   82    8    CHN    2020   \n",
       "64         0.986   28  239   390   90  3.84  243   76    3    CIN    2020   \n",
       "65         0.981   42  161   528   63  5.59  275   84   16    COL    2020   \n",
       "66         0.982   29  228   523  118  3.02  349   97    6    LAN    2020   \n",
       "67         0.981   51  191   472   60  4.86  263   82    5    MIA    2020   \n",
       "68         0.984   15  221   429   75  4.16  247   83    5    MIL    2020   \n",
       "69         0.985   20  197   551   86  4.98  286  106    7    NYN    2020   \n",
       "70         0.983   35  229   500   82  5.14  306   90   10    PHI    2020   \n",
       "71         0.978   16  167   425   59  4.68  219   76    6    PIT    2020   \n",
       "72         0.985   55  204   506   95  3.86  325  103   12    SDN    2020   \n",
       "73         0.980   19  195   532   81  4.64  299  107   14    SFN    2020   \n",
       "74         0.983   18  205   410   51  3.90  240   73    7    SLN    2020   \n",
       "75         0.981   33  192   519   66  5.09  293  112   12    WAS    2020   \n",
       "\n",
       "          LgWin  probability_Lg_Win  \n",
       "new_index                            \n",
       "1             N        2.062917e-04  \n",
       "2             N        1.684725e-04  \n",
       "3             Y        9.472162e-02  \n",
       "4             N        6.352732e-05  \n",
       "5             N        7.762760e-03  \n",
       "6             N        4.712927e-02  \n",
       "7             N        4.623801e-03  \n",
       "8             N        3.851510e-05  \n",
       "9             N        4.791355e-02  \n",
       "10            N        1.376867e-04  \n",
       "11            N        1.202732e-03  \n",
       "12            N        3.642738e-04  \n",
       "13            N        4.005748e-02  \n",
       "14            N        2.690497e-02  \n",
       "15            N        6.685922e-02  \n",
       "16            N        7.852897e-02  \n",
       "17            N        8.409917e-04  \n",
       "18            N        5.934635e-02  \n",
       "19            N        1.214681e-04  \n",
       "20            N        1.099750e-02  \n",
       "21            Y        8.584717e-02  \n",
       "22            N        9.543002e-04  \n",
       "23            N        4.266471e-03  \n",
       "24            N        6.383359e-04  \n",
       "25            N        2.067953e-03  \n",
       "26            N        2.001047e-03  \n",
       "27            N        1.160722e-04  \n",
       "28            N        2.204180e-04  \n",
       "29            N        1.555201e-02  \n",
       "30            N        5.940452e-02  \n",
       "31            N        6.932616e-03  \n",
       "32            N        3.586933e-02  \n",
       "33            N        5.702186e-02  \n",
       "34            N        2.646148e-04  \n",
       "35            N        4.773617e-03  \n",
       "36            Y        9.205669e-02  \n",
       "37            N        6.398028e-05  \n",
       "38            N        2.756757e-02  \n",
       "39            N        1.796544e-03  \n",
       "40            N        1.342475e-03  \n",
       "41            N        9.030312e-03  \n",
       "42            N        2.129972e-04  \n",
       "43            N        1.053423e-03  \n",
       "44            N        4.696625e-02  \n",
       "45            N        4.727721e-03  \n",
       "46            N        1.088586e-02  \n",
       "47            N        2.264199e-02  \n",
       "48            N        3.066266e-02  \n",
       "49            N        3.302515e-03  \n",
       "50            N        1.111597e-03  \n",
       "51            N        9.841037e-02  \n",
       "52            N        1.108356e-04  \n",
       "53            N        4.216343e-03  \n",
       "54            N        2.735195e-02  \n",
       "55            N        4.606992e-03  \n",
       "56            N        1.841962e-03  \n",
       "57            N        7.430613e-04  \n",
       "58            N        1.969606e-03  \n",
       "59            N        3.292796e-02  \n",
       "60            Y        2.166259e-02  \n",
       "61            N        1.797345e-07  \n",
       "62            N        2.289763e-06  \n",
       "63            N        1.454528e-06  \n",
       "64            N        1.679467e-06  \n",
       "65            N        1.921992e-08  \n",
       "66            Y        2.891421e-04  \n",
       "67            N        7.790432e-08  \n",
       "68            N        8.181820e-07  \n",
       "69            N        2.679697e-07  \n",
       "70            N        1.332976e-07  \n",
       "71            N        1.515237e-07  \n",
       "72            N        8.795061e-06  \n",
       "73            N        7.742470e-07  \n",
       "74            N        1.235227e-06  \n",
       "75            N        1.272267e-07  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing LgWin column back to ['N', 'Y']; it was changed to [0,1] for classification\n",
    "final_data = combined.copy()\n",
    "final_data.LgWin = final_data.LgWin.replace([0, 1], ['N', 'Y'])\n",
    "final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
